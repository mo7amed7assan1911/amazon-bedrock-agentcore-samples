{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph v1.0 with AgentCore Memory Middlewares (Long-term Memory)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to integrate Amazon Bedrock AgentCore Memory capabilities with a conversational AI agent using **LangGraph v1.0** framework with the **new middleware system**. We'll focus on **long-term memory** retention across multiple conversation sessions - allowing an agent to extract and recall user preferences, dietary restrictions, and contextual information from past interactions.\n",
    "\n",
    "## Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                          |\n",
    "|:--------------------|:---------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Long-term Conversational                                                        |\n",
    "| Agent usecase       | Nutrition Assistant                                                              |\n",
    "| Agentic Framework   | LangGraph v1.0 (with Middlewares)                                               |\n",
    "| LLM model           | Anthropic Claude Haiku 4.5                                                     |\n",
    "| Tutorial components | AgentCore Long-term Memory, Custom Memory Strategies, `@before_model`/`@after_model` Middlewares |\n",
    "| Example complexity  | Intermediate                                                                     |\n",
    "\n",
    "You'll learn to:\n",
    "- Create AgentCore Memory with UserPreference custom-override strategy\n",
    "- Implement **`@before_model` and `@after_model` middlewares** for automatic memory storage and retrieval\n",
    "- Build a nutrition assistant that remembers user preferences across sessions\n",
    "- Use semantic search to retrieve relevant user context\n",
    "- Configure custom memory extraction and consolidation prompts\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: This tutorial uses the **new LangGraph v1.0 `create_agent`** with middlewares, replacing the deprecated `create_react_agent` with pre/post hooks.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"architecture.png\" width=\"65%\" />\n",
    "</div>\n",
    "\n",
    "### Scenario Context\n",
    "\n",
    "In this example, we'll create a **Nutrition Assistant** that can remember user context across multiple conversations, including dietary restrictions, favorite foods, cooking preferences, and health goals. The agent will automatically extract and store user preferences from conversations, then retrieve relevant context for future interactions to provide personalized nutrition advice.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- AWS account with appropriate permissions\n",
    "- AWS IAM role with appropriate permissions for AgentCore Memory\n",
    "- Access to Amazon Bedrock models\n",
    "\n",
    "Let's get started by setting up our environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries from https://github.com/langchain-ai/langchain-aws\n",
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedhassan/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "# Import LangGraph v1.0 components (NEW: create_agent replaces deprecated create_react_agent)\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import create_agent  # NEW v1.0 API\n",
    "from langchain.agents.middleware import before_model, after_model, AgentState  # NEW: Middleware decorators\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.runtime import Runtime\n",
    "import uuid\n",
    "\n",
    "\n",
    "region = os.getenv('AWS_REGION', 'us-east-1')\n",
    "logging.getLogger(\"nutrition-agent\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Memory components\n",
    "from langgraph_checkpoint_aws import AgentCoreMemoryStore, AgentCoreMemorySaver\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "from bedrock_agentcore.memory.constants import StrategyType\n",
    "\n",
    "# NEW: Using MemoryManager from starter toolkit (simpler API)\n",
    "from bedrock_agentcore_starter_toolkit.operations.memory.manager import MemoryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚úÖ MemoryManager initialized for region: us-west-2\n",
      "Created memory: Nutrition_Assistant-36rxhrA85h\n",
      "Created memory Nutrition_Assistant-36rxhrA85h, waiting for ACTIVE status...\n",
      "Memory Nutrition_Assistant-36rxhrA85h is now ACTIVE (took 156 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory resource is ACTIVE with ID: Nutrition_Assistant-36rxhrA85h\n"
     ]
    }
   ],
   "source": [
    "# Memory configuration\n",
    "memory_name = \"Nutrition_Assistant\"\n",
    "MODEL_ID = \"global.anthropic.claude-haiku-4-5-20251001-v1:0\"\n",
    "\n",
    "# Using MemoryManager for simpler memory creation (no IAM role required for built-in strategies)\n",
    "memory_manager = MemoryManager(region_name=region)\n",
    "\n",
    "memory = memory_manager.get_or_create_memory(\n",
    "    name=memory_name,\n",
    "    strategies=[\n",
    "        # Strategy 1: User Preferences (food preferences, dietary restrictions)\n",
    "        {\n",
    "            StrategyType.USER_PREFERENCE.value: {\n",
    "                \"name\": \"NutritionPreferences\",\n",
    "                \"description\": \"Captures user food preferences and dietary behavior\",\n",
    "                \"namespaces\": [\"nutrition/{actorId}/preferences\"],\n",
    "            }\n",
    "        },\n",
    "        # Strategy 2: Semantic Memory (factual information from conversations)\n",
    "        {\n",
    "            StrategyType.SEMANTIC.value: {\n",
    "                \"name\": \"NutritionFacts\",\n",
    "                \"description\": \"Stores factual information from conversations\",\n",
    "                \"namespaces\": [\"nutrition/{actorId}/facts\"],\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory_id = memory.get('id')\n",
    "print(f\"‚úÖ Memory resource is ACTIVE with ID: {memory_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Configuration Overview\n",
    "\n",
    "Our AgentCore Memory setup uses **built-in strategies** (no IAM role required):\n",
    "\n",
    "- **USER_PREFERENCE Strategy**: Automatically extracts user preferences from conversations\n",
    "- **SEMANTIC Strategy**: Stores factual information mentioned in conversations\n",
    "- **Namespaces**: \n",
    "  - `nutrition/{actorId}/preferences` - User food preferences\n",
    "  - `nutrition/{actorId}/facts` - Factual information\n",
    "\n",
    "The memory system will automatically process conversations to extract lasting user preferences while filtering out temporary or irrelevant information.\n",
    "\n",
    "> üí° **Tip**: For custom extraction/consolidation prompts, use `StrategyType.CUSTOM` with `MemoryClient` (requires `memory_execution_role_arn`).\n",
    "\n",
    "## Step 3: Initialize Memory Store and LLM\n",
    "\n",
    "Now we'll initialize the AgentCore Memory Store and our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized: global.anthropic.claude-haiku-4-5-20251001-v1:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Bedrock LLM\n",
    "llm = init_chat_model(MODEL_ID, model_provider=\"bedrock_converse\", region_name=region)\n",
    "\n",
    "# Optional: Initialize checkpointer for short-term memory (conversation continuity within session)\n",
    "# checkpointer = AgentCoreMemorySaver(memory_id=memory_id, region_name=region)\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Memory Middlewares (LangGraph v1.0)\n",
    "\n",
    "In LangGraph v1.0, the `create_react_agent` function with `pre_model_hook` and `post_model_hook` parameters is **deprecated**. The new approach uses **middleware decorators**:\n",
    "\n",
    "| Old (Deprecated) | New (v1.0) |\n",
    "|------------------|------------|\n",
    "| `create_react_agent(llm, pre_model_hook=..., post_model_hook=...)` | `create_agent(llm, middleware=[...])` |\n",
    "| Functions passed as parameters | Decorators: `@before_model`, `@after_model` |\n",
    "\n",
    "We'll create middlewares to automatically handle memory storage and retrieval:\n",
    "\n",
    "- **`@before_model`**: Retrieves relevant user preferences (based on semantic search) and adds context before LLM invocation\n",
    "- **`@after_model`**: Saves the conversation messages for long-term memory extraction\n",
    "\n",
    "### How Memory Processing Works\n",
    "\n",
    "1. Messages are saved to AgentCore Memory with actor_id and session_id\n",
    "2. The custom strategy processes conversations to extract nutrition preferences\n",
    "3. Extracted preferences are stored in the `{actorId}/preferences` namespace\n",
    "4. Future conversations can search and retrieve relevant preferences for context\n",
    "\n",
    "**Note**: LangChain message types are converted under the hood by the store to AgentCore Memory message types so that they can be properly extracted to long term memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Middlewares created: retrieve_from_memory, save_to_memory\n"
     ]
    }
   ],
   "source": [
    "# Initialize MemoryClient for direct memory operations\n",
    "memory_client = MemoryClient(region_name=region)\n",
    "\n",
    "# Global variables for memory context (set before agent invocation)\n",
    "ACTOR_ID = \"default_user\"\n",
    "SESSION_ID = \"default_session\"\n",
    "\n",
    "BASE_PROMPT = \"\"\"You are a helpful nutrition assistant. You remember user preferences and provide personalized advice.\"\"\"\n",
    "\n",
    "def configure_memory_context(actor_id: str, session_id: str):\n",
    "    \"\"\"Configure the memory context for middlewares.\"\"\"\n",
    "    global ACTOR_ID, SESSION_ID\n",
    "    ACTOR_ID = actor_id\n",
    "    SESSION_ID = session_id\n",
    "\n",
    "\n",
    "@before_model\n",
    "def retrieve_from_memory(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"\n",
    "    BEFORE model middleware: Retrieve memories and inject into context.\n",
    "    \n",
    "    This replaces the deprecated pre_model_hook pattern from create_react_agent.\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Get last user message for semantic search\n",
    "    last_user_msg = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            last_user_msg = msg.content\n",
    "            break\n",
    "    \n",
    "    if not last_user_msg:\n",
    "        return None\n",
    "    \n",
    "    # Search memories using MemoryClient\n",
    "    memory_context = []\n",
    "    \n",
    "    # Search preferences namespace\n",
    "    try:\n",
    "        prefs = memory_client.retrieve_memories(\n",
    "            memory_id=memory_id,\n",
    "            namespace=f\"nutrition/{ACTOR_ID}/preferences\",\n",
    "            query=last_user_msg,\n",
    "        )\n",
    "        for p in prefs[:3]:\n",
    "            if isinstance(p, dict):\n",
    "                content = p.get(\"content\", {})\n",
    "                text = content.get(\"text\", str(content)) if isinstance(content, dict) else str(content)\n",
    "                memory_context.append(f\"Preference: {text}\")\n",
    "    except Exception as e:\n",
    "        logging.debug(f\"Preference retrieval error: {e}\")\n",
    "    \n",
    "    # Search facts namespace\n",
    "    try:\n",
    "        facts = memory_client.retrieve_memories(\n",
    "            memory_id=memory_id,\n",
    "            namespace=f\"nutrition/{ACTOR_ID}/facts\",\n",
    "            query=last_user_msg,\n",
    "        )\n",
    "        for f in facts[:3]:\n",
    "            if isinstance(f, dict):\n",
    "                content = f.get(\"content\", {})\n",
    "                text = content.get(\"text\", str(content)) if isinstance(content, dict) else str(content)\n",
    "                memory_context.append(f\"Fact: {text}\")\n",
    "    except Exception as e:\n",
    "        logging.debug(f\"Fact retrieval error: {e}\")\n",
    "    \n",
    "    # Inject memories into system prompt\n",
    "    if memory_context:\n",
    "        logging.info(f\"üìö Found {len(memory_context)} memories for {ACTOR_ID}\")\n",
    "        enhanced_prompt = BASE_PROMPT + \"\\n\\nWhat you know about this user:\\n\" + \"\\n\".join(memory_context)\n",
    "        new_msgs = [SystemMessage(content=enhanced_prompt)] + [m for m in messages if not isinstance(m, SystemMessage)]\n",
    "        return {\"messages\": new_msgs}\n",
    "    else:\n",
    "        logging.info(f\"üì≠ No memories found for {ACTOR_ID}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "@after_model\n",
    "def save_to_memory(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"\n",
    "    AFTER model middleware: Save conversation to memory.\n",
    "    \n",
    "    This replaces the deprecated post_model_hook pattern from create_react_agent.\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Extract latest conversation turn\n",
    "    human_msg, ai_msg = None, None\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage) and ai_msg is None:\n",
    "            ai_msg = msg.content\n",
    "        elif isinstance(msg, HumanMessage) and human_msg is None:\n",
    "            human_msg = msg.content\n",
    "        if human_msg and ai_msg:\n",
    "            break\n",
    "    \n",
    "    # Save to AgentCore Memory\n",
    "    if human_msg and ai_msg:\n",
    "        try:\n",
    "            memory_client.create_event(\n",
    "                memory_id=memory_id,\n",
    "                actor_id=ACTOR_ID,\n",
    "                session_id=SESSION_ID,\n",
    "                messages=[\n",
    "                    (human_msg, \"USER\"),\n",
    "                    (ai_msg, \"ASSISTANT\"),\n",
    "                ]\n",
    "            )\n",
    "            logging.info(f\"üíæ Saved conversation to memory for {ACTOR_ID}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Memory save error: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Middlewares created: retrieve_from_memory, save_to_memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the LangGraph v1.0 Agent\n",
    "\n",
    "Now we'll create our nutrition assistant agent using **LangGraph v1.0's `create_agent`** with our memory middlewares integrated.\n",
    "\n",
    "### Key Differences from Deprecated Approach\n",
    "\n",
    "```python\n",
    "# OLD (Deprecated)\n",
    "graph = create_react_agent(\n",
    "    llm,\n",
    "    store=store,\n",
    "    tools=[],\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    post_model_hook=post_model_hook\n",
    ")\n",
    "\n",
    "# NEW (v1.0)\n",
    "graph = create_agent(\n",
    "    llm,\n",
    "    tools=[],\n",
    "    middleware=[retrieve_from_memory, save_to_memory]\n",
    ")\n",
    "```\n",
    "\n",
    "**Note**: For custom agent implementations, the middlewares can be composed and extended as needed for any workflow following this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with LangGraph v1.0 create_agent and middlewares\n",
    "graph = create_agent(\n",
    "    llm,\n",
    "    tools=[],  # No additional tools needed for this example\n",
    "    middleware=[retrieve_from_memory, save_to_memory],  # NEW: Middleware pattern!\n",
    "    checkpointer=InMemorySaver(),  # For conversation state management\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Agent Runtime\n",
    "\n",
    "We need to configure the agent with unique identifiers for the user and session. These IDs are crucial for memory organization and retrieval.\n",
    "\n",
    "### Graph Invoke Input\n",
    "We only need to pass the newest user message in as an argument `inputs`. This could include other state variables as well but for the simple `create_react_agent`, we only need messages.\n",
    "\n",
    "### LangGraph RuntimeConfig\n",
    "In LangGraph, config is a `RuntimeConfig` that contains attributes that are necessary at invocation time, for example user IDs or session IDs. For the `AgentCoreMemorySaver`, `thread_id` and `actor_id` must be set in the config. For instance, your AgentCore invocation endpoint could assign this based on the identity or user ID of the caller. You can read additional [documentation here](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configured for actor=test-user, session=test-session\n"
     ]
    }
   ],
   "source": [
    "actor_id = \"test-user\"\n",
    "session_id = \"test-session\"\n",
    "\n",
    "# Configure memory context for middlewares\n",
    "configure_memory_context(actor_id, session_id)\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": session_id,  # REQUIRED: This maps to Bedrock AgentCore session_id under the hood\n",
    "        \"actor_id\": actor_id,     # REQUIRED: This maps to Bedrock AgentCore actor_id under the hood\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Configured for actor={actor_id}, session={session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test the Agent\n",
    "\n",
    "Let's test our nutrition assistant by having a conversation about food preferences. The agent will automatically extract and store user preferences for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "Hey there! Im cooking one of my favorite meals tonight, salmon with rice and veggies (healthy). Has\n",
      "great macros for my weightlifting competition that is coming up. What can I add to this dish to make it taste better\n",
      "and also improve the protein and vitamins I get?\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "# Great meal choice! Here are some additions that boost both flavor and nutrition:\n",
      "\n",
      "## Protein boosters:\n",
      "- **Greek yogurt or cottage cheese** - mix into a sauce for creaminess + extra protein\n",
      "- **Sesame seeds or hemp seeds** - sprinkle on top for texture and complete amino acids\n",
      "- **Nutritional yeast** - umami flavor + B vitamins\n",
      "\n",
      "## Flavor enhancers that add nutrition:\n",
      "- **Soy sauce or tamari** - savory depth + minerals\n",
      "- **Lemon/lime juice** - brightens everything, aids mineral absorption\n",
      "- **Garlic & ginger** - antimicrobial + anti-inflammatory\n",
      "- **Miso paste** - umami bomb + probiotics\n",
      "\n",
      "## Vitamin-packed toppings:\n",
      "- **Microgreens** - concentrated nutrients + color\n",
      "- **Seaweed snack** - iodine + trace minerals\n",
      "- **Avocado or olive oil drizzle** - healthy fats for vitamin absorption\n",
      "- **Fresh herbs** (dill, cilantro, parsley) - minimal calories, micronutrients\n",
      "\n",
      "## Veggie tip:\n",
      "Make sure those veggies include **cruciferous** ones (broccoli, bok choy) or **leafy greens** for additional vitamins K and folate.\n",
      "\n",
      "**Simple winning combo:** Sesame-crusted salmon with lemon-ginger sauce, served over rice + roasted broccoli with a drizzle of sesame oil.\n",
      "\n",
      "What veggies are you planning to use?\n"
     ]
    }
   ],
   "source": [
    "# Helper function to pretty print agent output while running\n",
    "def run_agent(query: str, config: RunnableConfig):\n",
    "    printed_ids = set()\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            for msg in event[\"messages\"]:\n",
    "                # Check if we've already printed this message\n",
    "                if id(msg) not in printed_ids:\n",
    "                    msg.pretty_print()\n",
    "                    printed_ids.add(id(msg))\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Hey there! Im cooking one of my favorite meals tonight, salmon with rice and veggies (healthy). Has\n",
    "great macros for my weightlifting competition that is coming up. What can I add to this dish to make it taste better\n",
    "and also improve the protein and vitamins I get?\n",
    "\"\"\"\n",
    "\n",
    "run_agent(prompt, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What was stored?\n",
    "As you can see, the model does not yet have any insight into our preferences or dietary restrictions.\n",
    "\n",
    "For this implementation with `@before_model` and `@after_model` middlewares, two messages were stored here. The first message from the user and the response from the AI model were both stored as conversational events in AgentCore Memory. It may take a few moments for the long term memories to be extracted, so retry after a few seconds if nothing is found the first try.\n",
    "\n",
    "These messages were then extracted to AgentCore long term memory in our fact and user preferences namespaces. In fact, we can check the store ourselves to verify what has been stored there so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking memories for: test-user\n",
      "============================================================\n",
      "\n",
      "üìã PREFERENCES:\n",
      "  ‚Ä¢ {\"context\":\"The user explicitly stated that salmon with rice and veggies is one of their favorite meals.\",\"preference\":\"Likes salmon with rice and vegetables\",\"categories\":[\"food\",\"meals\"]}\n",
      "  ‚Ä¢ {\"context\":\"The user expressed interest in enhancing vitamin content in their meal.\",\"preference\":\"Interested in increasing vitamin intake\",\"categories\":[\"nutrition\",\"health\"]}\n",
      "  ‚Ä¢ {\"context\":\"The user specifically asked for ways to improve protein content in their meal.\",\"preference\":\"Interested in increasing protein intake\",\"categories\":[\"nutrition\",\"fitness\"]}\n",
      "\n",
      "üìö FACTS:\n",
      "  ‚Ä¢ The user is preparing for an upcoming weightlifting competition.\n",
      "  ‚Ä¢ The user considers the salmon, rice, and veggies meal to be healthy.\n",
      "  ‚Ä¢ The user is concerned about the macronutrient content of their meals for their weightlifting competition.\n"
     ]
    }
   ],
   "source": [
    "# Check what's been stored in memory\n",
    "print(f\"üîç Checking memories for: {actor_id}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìã PREFERENCES:\")\n",
    "prefs = memory_client.retrieve_memories(\n",
    "    memory_id=memory_id,\n",
    "    namespace=f\"nutrition/{actor_id}/preferences\",\n",
    "    query=\"food preferences\",\n",
    ")\n",
    "for p in prefs[:5]:\n",
    "    text = p.get(\"content\", {}).get(\"text\", str(p)) if isinstance(p, dict) else str(p)\n",
    "    print(f\"  ‚Ä¢ {text}\")\n",
    "if not prefs:\n",
    "    print(\"  (none yet - memories take ~30s to extract)\")\n",
    "\n",
    "print(\"\\nüìö FACTS:\")\n",
    "facts = memory_client.retrieve_memories(\n",
    "    memory_id=memory_id,\n",
    "    namespace=f\"nutrition/{actor_id}/facts\",\n",
    "    query=\"user facts\",\n",
    ")\n",
    "for f in facts[:5]:\n",
    "    text = f.get(\"content\", {}).get(\"text\", str(f)) if isinstance(f, dict) else str(f)\n",
    "    print(f\"  ‚Ä¢ {text}\")\n",
    "if not facts:\n",
    "    print(\"  (none yet - memories take ~30s to extract)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent access to the store\n",
    "\n",
    "**Note** - since AgentCore memory processes these events in the background, it may take a few seconds for the memory to be extracted and embedded to long term memory retrieval.\n",
    "\n",
    "Great! Now we have seen that long term memories were extracted to our namespaces based on the earlier messages in the conversation.\n",
    "\n",
    "Now, let's start a new session and ask about recommendations for what to cook for dinner. The agent can use the store to access the long term memories that were extracted to make a recommendation that the user will be sure to like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ New session: session-2\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Today's a new day, what should I make for dinner tonight?\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful nutrition assistant. You remember user preferences and provide personalized advice.\n",
      "\n",
      "What you know about this user:\n",
      "Preference: {\"context\":\"The user mentioned they're cooking salmon with rice and veggies, which they described as healthy.\",\"preference\":\"Enjoys healthy meals\",\"categories\":[\"food\",\"nutrition\"]}\n",
      "Preference: {\"context\":\"The user explicitly stated that salmon with rice and veggies is one of their favorite meals.\",\"preference\":\"Likes salmon with rice and vegetables\",\"categories\":[\"food\",\"meals\"]}\n",
      "Preference: {\"context\":\"The user expressed interest in enhancing vitamin content in their meal.\",\"preference\":\"Interested in increasing vitamin intake\",\"categories\":[\"nutrition\",\"health\"]}\n",
      "Fact: The user is cooking salmon with rice and veggies for dinner.\n",
      "Fact: The user considers the salmon, rice, and veggies meal to be healthy.\n",
      "Fact: The user is concerned about the macronutrient content of their meals for their weightlifting competition.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "# Dinner Ideas for Tonight\n",
      "\n",
      "Based on what I know about you, here are some great options:\n",
      "\n",
      "## Your Go-To Choice\n",
      "**Salmon with rice and veggies** ‚Äì Since this is one of your favorites and you consider it healthy, it's always a solid pick! Plus, it aligns well with your weightlifting competition goals (excellent protein from salmon, carbs from rice for energy).\n",
      "\n",
      "## Variations to Keep Things Fresh\n",
      "- **Switch up the veggies** ‚Äì Try roasted broccoli, asparagus, or sweet potato instead of your usual sides to boost vitamin variety\n",
      "- **Different grain** ‚Äì Swap rice for quinoa or farro for added protein and nutrients\n",
      "- **Herb-crusted salmon** ‚Äì Add dill, lemon, or garlic to boost flavor and micronutrient content\n",
      "\n",
      "## Pro Tip for Your Competition\n",
      "Given your weightlifting focus, this salmon + rice + veggies combo is *ideal* because it gives you:\n",
      "- **Protein** from salmon (muscle recovery)\n",
      "- **Carbs** from rice (energy for training)\n",
      "- **Micronutrients** from veggies (recovery support)\n",
      "\n",
      "**Quick question:** Are you looking to mix things up today, or would you like tips on optimizing your usual salmon meal for your competition prep?\n"
     ]
    }
   ],
   "source": [
    "# New session with same user\n",
    "session_id = \"session-2\"\n",
    "\n",
    "# Update memory context for new session\n",
    "configure_memory_context(actor_id, session_id)\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": session_id,  # New session ID\n",
    "        \"actor_id\": actor_id,     # Same actor ID\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ New session: {session_id}\")\n",
    "run_agent(\"Today's a new day, what should I make for dinner tonight?\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "As you can see, the agent received context from the `@before_model` middleware (user preferences namespace search) and was able to search on its own for long term memories in the fact namespace to create a comprehensive answer for the user.\n",
    "\n",
    "## Summary: LangGraph v1.0 Migration\n",
    "\n",
    "| Old (Deprecated) | New (v1.0) |\n",
    "|------------------|------------|\n",
    "| `from langgraph.prebuilt import create_react_agent` | `from langchain.agents import create_agent` |\n",
    "| `pre_model_hook=..., post_model_hook=...` | `middleware=[...]` |\n",
    "| Functions as parameters | Decorators: `@before_model`, `@after_model` |\n",
    "\n",
    "The AgentCoreMemoryStore is very flexible and can be implemented in a variety of ways, including `@before_model`/`@after_model` middlewares or just tools themselves with store operations. Used alongside the AgentCoreMemorySaver for checkpointing, both full conversational state and long term insights can be combined to form a complex and intelligent agent system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
